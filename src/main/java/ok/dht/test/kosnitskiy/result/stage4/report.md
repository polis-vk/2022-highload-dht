ENVIRONMENT-DESCRIPTION
Тестирование производилось на 6-ядерном, 6-поточном процессоре. 
Были запущены 3 ноды, дожидались ответа от двух нод, реплецировали на все три

GET-DESCRIPTION
Для тестирования было решено использовать ключи, закрытые сверху еще 1.5 гигами информации с другими ключами. 
Это стандартный юзкейс для нашей базы данных - ключи существуют, однако чтобы их достать прийдется порыться по файлам
и выкопать их их глубины

PUT-DESCRIPTION
Для PUT мы генерировали разные ключи, чтобы гарантировать, что размер DAO будет увеличиваться и не будет
перезаписываться одно и тоже значение кучу раз.
Так же мы стали генерировать строки разной длинны чтобы несколько усложить бинарный поиск по получившимся файлам

WRK-PUT
◆ utils git:(stage4) ✗ ❯❯❯ wrk -d 20s -t 4 -c 64 -R 2500 -s put-different.lua --latency "http://localhost:19230"
Running 20s test @ http://localhost:19230
4 threads and 64 connections
Thread calibration: mean lat.: 9.028ms, rate sampling interval: 41ms
Thread calibration: mean lat.: 10.560ms, rate sampling interval: 43ms
Thread calibration: mean lat.: 8.740ms, rate sampling interval: 40ms
Thread calibration: mean lat.: 9.451ms, rate sampling interval: 43ms
Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency    13.92ms   16.00ms 111.17ms   86.10%
Req/Sec   635.24    183.89     1.51k    73.31%
Latency Distribution (HdrHistogram - Recorded Latency)
50.000%    7.74ms
75.000%   18.09ms
90.000%   34.78ms
99.000%   77.95ms
99.900%   99.26ms
99.990%  105.28ms
99.999%  111.23ms
100.000%  111.23ms

Тестирование производилось при нагрузке node 0
Легко заметить, что наша скорость добавления данных упала примерно в 3 раза. Это следовало ожидать, ведь мы отправляем
запросы на добавление сразу трем нодам, дожидаясь ответа от двух их них. Так же потеря производительности связана
с необходимостью теперь хранить таймстемпы, иметь блокирующую очередь пришедших результатов, а так же проверками,
необходимыми для вычисления болеющих нод.



WRK-GET
◆ utils git:(stage4) ✗ ❯❯❯ wrk -d 20s -t 4 -c 64 -R 1700 -s get-random.lua --latency "http://localhost:19230"
Running 20s test @ http://localhost:19230
4 threads and 64 connections
Thread calibration: mean lat.: 43.840ms, rate sampling interval: 205ms
Thread calibration: mean lat.: 44.679ms, rate sampling interval: 204ms
Thread calibration: mean lat.: 44.018ms, rate sampling interval: 204ms
Thread calibration: mean lat.: 43.304ms, rate sampling interval: 201ms
Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency    28.39ms   33.34ms 135.94ms   84.00%
Req/Sec   426.66     45.95   539.00     75.65%
Latency Distribution (HdrHistogram - Recorded Latency)
50.000%   13.81ms
75.000%   39.87ms
90.000%   78.27ms
99.000%  124.74ms
99.900%  130.11ms
99.990%  133.50ms
99.999%  136.06ms
100.000%  136.06ms


Легко заметить, что наша скорость получения данных упала примерно в 3 раза тоже. Это следовало ожидать, ведь мы отправляем
запросы на добавление сразу трем нодам, дожидаясь ответа от двух их них. Так же потеря производительности связана
с необходимостью теперь хранить таймстемпы, иметь блокирующую очередь пришедших результатов, а так же проверками,
необходимыми для вычисления болеющих нод.

В обоих случаях часть проблем вызвано таймстемпами, как одна из оптимизаций - хорошо было бы оптимизировать их хранение,
что позволит не только экономить место, но и увеличит производительность в целом.

Благодаря флеймграфам видим, что логгеры тоже тратят значительное количество времени. Сейчас без них никак, но в конечной
реализации надо будет не забыть их отключить.
Так же много времени теперь занимают локи, это связано с сильным увеличением количества очередей, которые мы используем
в нашей реализации.

По сравнению с прошлым стейджем сильно выросло количество аллокаций - это понятно, так как теперь у нас при текущей
настройки все данные пытаются положиться на каждую из нод.

Так же куда больше времени теперь уходит на флеймграффе на получение и обработку ответов, оно и понятно, ведь теперь
мы отправляем куда больше запросов и активно общаемся между самими нодами

Спасибо за прочтение!