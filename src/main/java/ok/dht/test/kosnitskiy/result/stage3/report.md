ENVIRONMENT-DESCRIPTION
Тестирование производилось на 6-ядерном, 6-поточном процессоре. 
Были запущены 3 ноды, для распределения нагрузки между ними было решено использовать murmur алгоритм хеширования,
так как из протестированных мной он предложил самое адекватное распределение информации по нодам.

GET-DESCRIPTION
Для тестирования было решено использовать ключи, закрытые сверху еще 1.5 гигами информации с другими ключами. 
Это стандартный юзкейс для нашей базы данных - ключи существуют, однако чтобы их достать прийдется порыться по файлам
и выкопать их их глубины

PUT-DESCRIPTION
Для PUT мы генерировали разные ключи, чтобы гарантировать, что размер DAO будет увеличиваться и не будет
перезаписываться одно и тоже значение кучу раз.
Так же мы стали генерировать строки разной длинны чтобы несколько усложить бинарный поиск по получившимся файлам

WRK-PUT
◆ utils git:(stage3) ✗ ❯❯❯ wrk -d 20s -t 4 -c 64 -R 7000 -s put-different.lua --latency "http://localhost:19230"
Running 20s test @ http://localhost:19230
4 threads and 64 connections
Thread calibration: mean lat.: 4.633ms, rate sampling interval: 20ms
Thread calibration: mean lat.: 4.618ms, rate sampling interval: 20ms
Thread calibration: mean lat.: 4.554ms, rate sampling interval: 20ms
Thread calibration: mean lat.: 6.975ms, rate sampling interval: 26ms
Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency     5.14ms    3.90ms  31.76ms   77.87%
Req/Sec     1.79k   299.57     2.89k    69.65%
Latency Distribution (HdrHistogram - Recorded Latency)
50.000%    4.16ms
75.000%    6.87ms
90.000%   10.31ms
99.000%   18.50ms
99.900%   26.78ms
99.990%   29.79ms
99.999%   31.63ms
100.000%   31.77ms

Тестирование производилось при нагрузке node 0
Легко заметить, что наша скорость добавления данных в базу упала сразу в 10 раз, этого следовало ожидать, учитывая
что раньше задачка положить что-то в базу была легкой, а теперь ноде приходится не только высчитывать хеш у каждого
из ключей, что довольно неприятная операция, так еще и потенциально идти в другую ноду и ждать ответа об успешном
положении ключа из нее.
Тут главное улучшение которое можно сделать в данный момент - это алгоритм хождения от одной ноды к другой, во-первых
http - точно не самое быстрое решение, которое не особо рационально использовать в нашем случае, но мы делаем это
ради скорости, во-вторых хорошо бы было написать алгоритм, который бы смотрел, что к нам не просто пришел запрос,
а запрос от нашей ноды - товарища, и в таком случае не считать хеш и не делать никаких доп проверок, а сразу класть
ключ в базу, так будет намного быстрее. Сейчас же хеш высчитывается два раза и мы не различаем себя от чужого.
Так же различие 50 с даже 90 перцентилями стало намного сильнее, это связано с тем, что теперь у нас намного больше
долгих запросов и они встречаются не только в 99 перцентиле - это запросы, которые ходят в другую ноду
Так же падение производительности связано с тем, что все наши ноды работают на одной машине



WRK-GET
◆ utils git:(stage3) ✗ ❯❯❯ wrk -d 20s -t 4 -c 64 -R 4000 -s get-random.lua --latency "http://localhost:19230"
Running 20s test @ http://localhost:19230
4 threads and 64 connections
Thread calibration: mean lat.: 2.509ms, rate sampling interval: 10ms
Thread calibration: mean lat.: 2.479ms, rate sampling interval: 10ms
Thread calibration: mean lat.: 2.459ms, rate sampling interval: 10ms
Thread calibration: mean lat.: 2.451ms, rate sampling interval: 10ms
Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency     2.06ms    1.52ms  17.25ms   83.24%
Req/Sec     1.05k   192.46     2.00k    80.42%
Latency Distribution (HdrHistogram - Recorded Latency)
50.000%    1.64ms
75.000%    2.46ms
90.000%    3.92ms
99.000%    7.74ms
99.900%   11.36ms
99.990%   15.08ms
99.999%   17.26ms
100.000%   17.26ms


Для гета с учетом того, что потеря произоводительности связана с запуском всех 3 нод на одной машине почти незаметна.
Это связано с тем, что в гете мы не только проигрываем в скорости как в путе(по тем же причинам), но еще и существенно
выигрываем. Дело в том, что теперь файлов на каждой ноде намного меньше, ведь теперь они распределены по 3 нодам, а
до этого ютились на одной. Это дает нам большое преимущество!

Стоит отметить, что так же стоит учитывать возможное наличие проблемы с больной нодой, когда одна из нод очень долго
отвечает на запросы и фактически не справляется с направленным на нее потоком, а воркеры остальных нод застряли на
ожидании ответа от оной. Наивный подход решения такой проблемы - сделать отдельный пулл воркеров под каждую из нод,
однако проблема с таким подходом - если у нас в кластере огромное количество нод - 300+, то создание такой немалой
кучи потоков начинает стоить нам довольно много. Более изощренный подход - оставить один пулл, однако ограничить
количество потоков, которые могут идти на каждую из нод. Такой подход намного привлекательнее, но и намного сложнее
в реализации.

Так же стоит проговорить про выбор хеш функции. Я выбрал самую простую хеш функцию - процент не просто так, а по той
причине, что столь простая реализация требует от нас считать лишь один хеш код для каждого ключа, а рандеву, например,
требует от нас считать хеш код столько раз, сколько у нас нод для каждого ключа, что может критически порезать производительность
на большом количестве нод. Даже на трех прирост скорости весьма заметен. А с учетом того, что мы считаем нашу базу с
фиксированным количеством нод, особых проблем такая реализация нам не доставит.

По флеймграфам можно заметить, что теперь у нас график куда более разряженный, много полностью белых квадратов, а красные
появляются только время от времени. Это связано с тем, что теперь зачастую базы не работают, а просто ждут ответа от нод,
на которые они проксировали запросы. По этой причине возможно было бы хорошей идеей дополнительно иметь пулл потоков чисто
под запросы, которые будут исполняться локально, это помогло бы занять то процессорное время, которое на данный момент
пустое

Спасибо за прочтение!