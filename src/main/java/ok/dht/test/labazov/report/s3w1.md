# Шардирования

На данном этапе мы реализовали шардирование ключей между нодами, а также добавили в них поддержку
проксирования запросов. 

## Хэш-функция
В моей реализации я применил криптографическую хэш-функцию SHA-256 от строкового ключа, чтобы
добиться хорошего распределения по нодам, а также противостоять DoS атакам на отдельные ноды при помощи нахождения коллизий.
Последним свойством можно было бы пожертвовать в угоду скорости, выбрав другую функцию (например, MurmurHash3),
но как мы потом увидим на графике, CPU-time хэш-функции <1%. Возможно, это из-за малого размера ключа, но в реальной жизни
он обычно такой и есть, поэтому всё хорошо.

## Наполнение базы через `PUT`
```text
$ ./wrk -c 64 -t 4 -R 8000 -d 15 -s ~/study-files/highload/lua/put.lua http://localhost:19234

Running 15s test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 1.484ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.213ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.465ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.500ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     0.90ms  403.50us   5.66ms   64.69%
    Req/Sec     2.14k   359.26     3.56k    75.63%
  117784 requests in 15.00s, 7.53MB read
  Non-2xx or 3xx responses: 71
Requests/sec:   7853.04
Transfer/sec:    513.84KB
```

Для тестов пришлось понизить RPS, чтобы сервер перестал захлёбываться. Как мы видим, даже при 7k RPS, у нас иногда проскакивают
ошибки на запросы. можно выдвинуть две теории:
* Мы тестируем всё локально, и ресурсов просто не хватает.
* Используемый HTTP-клиент -- синхронный, а асинхронную реализацию будем рассматривать на следующей лекции.

## Чтение базы через `GET`
```text
$ ./wrk -c 64 -t 4 -R 8000 -d 15 -s ~/study-files/highload/lua/get.lua http://localhost:19234

Running 15s test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 0.828ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 0.928ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 0.881ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 0.835ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     0.87ms  519.08us  12.78ms   79.76%
    Req/Sec     2.14k   155.67     4.00k    79.38%
  119721 requests in 15.00s, 85.74MB read
  Non-2xx or 3xx responses: 530
Requests/sec:   7981.80
Transfer/sec:      5.72MB
```

Ситуация полностью аналогична предыдущему пункту.

## Анализ Flamegraph

* Как мы видим, на проксирование мы тратим 25-30% процессорного времени (в 10-30 раз больше локальной обработки для
`GET` и `PUT` соответственно). Причём большую часть времени мы просто стоим в сисколлы. Про асинхронность было сказано
уже выше, мы её хотим попробовать.
* Профилировка аллокаций показывает, что мы стали выделять много памяти на посылку HTTP-запросов. Конечно, мы же теперь
ещё и прокси.
* Нагрузка на блокировки не изменилось, и это ожидаемо.

# Выводы
При появлении шардирования у нас возникли серьёзные проблемы с падением RPS и таймаутами. Анализ показывает, что
проблема состоит в HTTP-клиенте, который очень плохо работает. В качестве решений этой проблемы вижу два варианта:
* Не заниматься проксированием самостоятельно, пусть за нас это делает gateway-прокси, которая умно между нодами
распределит запросы.
* Пусть клиент ходит в нужные ноды сам, но это error-prone, малорасширяемо и неинтересно, мы так делать не будем.
* Сделать проксирование асинхронным и посмотреть, что получится.