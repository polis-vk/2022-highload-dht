# Этап 3. Шардирование

<hr>

## Нагрузочное тестирование с использованием wrk2

<hr>

В качестве алгоритма распределения данных по узлам был выбран `Rendezvous hashing`. Добавлено разделение на два сервиса, обрабатывающих запросы, один в рамках текущего узла, второй для общения с другими узлами. Поскольку теперь необходимо считать *`N`* хэшей, то можно оптимизировать этот подсчет и вычислять значения не за линейное время, а за логарифм, используя *`Skeleton вариацию`*, суть которой заключается в виртуальном объединении узлов в дерево. Что может стать неплохой оптимизацией, если кластер состоит из большого числа узлов, позволяя уменьшить асимптотическую сложность до *`O(log N)`*.

Для того, чтобы определить насколько эффективно распределяет данные по узлам `Rendezvous hashing`, было подсчитано количество значений на каждом узле, состоящем в кластере из трёх. Соотношение получилось `33.25%:33.24%:33.49%`, разброс всего в `0.2%` позволяет сделать вывод, что алгоритм оптимально справляется с этой задачей. 

### *PUT-запросы*

Для сравнения было проведено нагрузочное тестирование на кластерах из разного количества узлов: `[1, 2, 3, 5]`, нагрузка выдавалась с параметрами `-t4 -c4 -d30s -R15000`:

| Cluster size | < 2ms percentile | Latency avg |
| :----------: | :--------------: | :---------: |
|    *`1`*     |       98%        |   0.91ms    |
|    *`2`*     |       87%        |   1.51ms    |
|    *`3`*     |       86%        |    2.3ms    |
|    *`5`*     |       84%        |    2.7ms    |

Как видно из таблицы, сильное падение в производительности наблюдается только после того, как узлов становится больше, чем один. Связано это с тем, что теперь узлам необходимо общаться друг с другом по сети, что достаточно сильно сказывается на общем перфомансе. С добавлением количества узлов, производительность продолжает не критично падать. Это также связано с тем, что теперь вероятность клиенту попасть в нужную ноду становится ниже из-за повышения их количества, что в свою очередь вынуждает делать сетевые запросы. По сути, кластер из одного узла при рейте равном `15000` справляется с нагрузкой, но для кластеров из большего числа нод необходимо понижать рейт. Например, для кластера из трёх узлов оптимальным рейтом будет `-R11000`. 

<hr>

### GET-запросы

Аналогично рассмотрим, как показывают себя `GET` запросы для кластеров из различного количества узлов, параметры аналогичны тем, что были:


| Cluster size | < 2ms percentile | Latency avg |
| :----------: | :--------------: | :---------: |
|    *`1`*     |       91%        |   2.13ms    |
|    *`2`*     |       78%        |    4.5ms    |
|    *`3`*     |       76%        |    4.7ms    |
|    *`5`*     |       76%        |    5.9ms    |

В целом, ситуация идентичная `PUT-запросам`. Резкая просадка наблюдается, когда становится больше одного узла. Из-за того, что `GET`запросы показывают себя на этом рейте ещё хуже, чем `PUT`, в следствии того, что требуют значительно большего количества системных вызовов при поиске ключа, для них оптимальное значение рейта должно быть ещё ниже иначе кластер не будет совершенно справляться с нагрузкой и сильно проваливаться после достижения определённого порога. Для кластера из трёх нод оптимальным будет значение `-R7500`.

<hr>

## *Async-profiler*

При профилировании будет рассматривать варианты, когда кластер состоит из одного или трёх узлов, чтобы понять, как влияет шардирование на общий перфоманс.

<hr>

### *PUT-запросы*

<hr>

#### *CPU*

При добавлении шардирования распределение ресурсов очень сильно меняется. В кластере из одного узла большую часть процессорного времени занимает запись в сокет -`35%` (метод *write*), взятие задачи из очереди (метод *take*) - `15.26%` и постановка задачи в хвост очередь (метод *offer*) - `5.15%`. Теперь же добавляется запись в сокет занимает всего `6.64%` процессорного времени, постановка в очередь - `3.95%`,  постановка в очередь - `0.99%`,  теперь метод `send` у `HttpClient` занимает `10.48%`, который большую часть занимается обработкой `CompletableFuture` - `9.46%`.  Помимо этого добавляется `SequentialScheduler` - планировщик задач, который должны выполняться последовательно, используется в качестве средства синхронизации и занимает `10.01%` процессорного времени.

#### *ALLOC*

При добавлении узлов появляются аллокации при использовании `SequentialScheduler` - `18.81%`, большая часть которых возникает при обработке `CompletableFuture`. 

#### *LOCK*

Если раньше почти все блокировки происходили при добавлении/взятии задач в очередь, то теперь добавляются блокировки, которые необходимы для общения узлов между собой - `47.5%`. Помимо этого добавляются локи при работе `SelectorManager` - `30.15%`.

<hr>

### *GET-запросы*

<hr>

#### CPU

По сути, всё аналогично  *`put`* запросам. Добавляются задачи, которые ответственны за общение узлов по сети, которые занимают весомую часть процессорного времени, однако поиск данных на диске всё равно является доминирующим, требующим в первую очередь оптимизаций узким местом, для этого можно, например, проверять однозначность нахождения ключа на странице, проверяя первое и последнее значение для того, чтобы минимизировать количество системных вызовов при бинарном поиске. 

#### ALLOC

Точно так же добавляются аллокации при общении узлов между собой и занимают весомые `16.7%`.

#### LOCK

Появляются блокировки из-за общения по сети. Основная часть при работе `SelectorManager`.

<hr>

Перфоманс после добавления шардирования ожидаемо стал хуже из-за того, что теперь узлам необходимо контактировать друг с другом по сети. Оптимизации необходимые на уровне базы данных остаются всё теми же. Для улучшения модернизации хэширования нужно реализовать `Skeleton вариацию`, кроме того неплохо было бы знать о том является ли узёл доступным, справляется ли с нагрузкой, для этого нужно реализовать `circuit breaker`.





