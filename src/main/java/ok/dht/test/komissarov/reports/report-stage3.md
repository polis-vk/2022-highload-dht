# Этап 3. Шардирование

<hr>

## Нагрузочное тестирование с использованием wrk2

<hr>

В качестве алгоритма распределения данных по узлам был выбран `Rendezvous hashing`. Добавлено разделение на два сервиса, обрабатывающих запросы, один в рамках текущего узла, второй для общения с другими узлами. Поскольку теперь необходимо считать *`N`* хэшей, то можно оптимизировать этот подсчет и вычислять значения не за линейное время, а за логарифм, используя *`Skeleton вариацию`*, суть которой заключается в виртуальном объединении узлов в дерево. Что может стать неплохой оптимизацией, если кластер состоит из большого числа узлов, позволяя уменьшить асимптотическую сложность до *`O(log N)`*.

Для того, чтобы определить насколько эффективно распределяет данные по узлам `Rendezvous hashing`, было подсчитано количество значений на каждом узле, состоящем в кластере из трёх. Соотношение получилось `33.25%:33.24%:33.49%`, разброс всего в `0.2%` позволяет сделать вывод, что алгоритм оптимально справляется с этой задачей. 

### *PUT-запросы*

Для сравнения было проведено нагрузочное тестирование на кластерах из разного количества узлов: `[1, 2, 3, 5]`, нагрузка выдавалась с параметрами `-t4 -c4 -d30s -R15000`:

| Cluster size | < 2ms percentile | Latency avg |
| :----------: | :--------------: | :---------: |
|    *`1`*     |       98%        |   0.91ms    |
|    *`2`*     |       87%        |   1.51ms    |
|    *`3`*     |       86%        |    2.3ms    |
|    *`5`*     |       84%        |    2.7ms    |

Как видно из таблицы, сильное падение в производительности наблюдается только после того, как узлов становится больше, чем один. Связано это с тем, что теперь узлам необходимо общаться друг с другом по сети, что достаточно сильно сказывается на общем перфомансе. С добавлением количества узлов, производительность продолжает не критично падать. Это также связано с тем, что теперь вероятность клиенту попасть в нужную ноду становится ниже из-за повышения их количества, что в свою очередь вынуждает делать сетевые запросы. По сути, кластер из одного узла при рейте равном `15000` справляется с нагрузкой, но для кластеров из большего числа нод необходимо понижать рейт. Например, для кластера из трёх узлов оптимальным рейтом будет `-R11000`. 

<hr>

### GET-запросы

Аналогично рассмотрим, как показывают себя `GET` запросы для кластеров из различного количества узлов, параметры аналогичны тем, что были:


| Cluster size | < 2ms percentile | Latency avg |
| :----------: | :--------------: | :---------: |
|    *`1`*     |       91%        |   2.13ms    |
|    *`2`*     |       78%        |    4.5ms    |
|    *`3`*     |       76%        |    4.7ms    |
|    *`5`*     |       76%        |    5.9ms    |

В целом, ситуация идентичная `PUT-запросам`. Резкая просадка наблюдается, когда становится больше одного узла. Из-за того, что `GET`запросы показывают себя на этом рейте ещё хуже, чем `PUT`, в следствии того, что требуют значительно большего количества системных вызовов при поиске ключа, для них оптимальное значение рейта должно быть ещё ниже иначе кластер не будет совершенно справляться с нагрузкой и сильно проваливаться после достижения определённого порога. Для кластера из трёх нод оптимальным будет значение `-R7500`.

<hr>

## *Async-profiler*

При профилировании будет рассматривать варианты, когда кластер состоит из одного или трёх узлов, чтобы понять, как влияет шардирование на общий перфоманс.

[]()