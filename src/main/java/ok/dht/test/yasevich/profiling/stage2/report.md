* Имеем lua скрипты [GET](../scripts/get.lua) и [PUT](../scripts/put.lua), генерирующие соответсвующее запросы со
  случайными ключами и значениями вида key n value m
* В БД изначально `130` файлов суммарным размером `1,3 Гб` (с записями вплоть до key 50кк, value 50kk)
* `5 Мб` flushThreshold
* Размер очереди `1000`

Прогрев за 30 секунд сервер программой wrk, запускаем профилирование put запросов, потом сразу же запускаем
профилирование get.
Профилировщик запустим в трёх режимах: cpu, alloc и lock:

`./profiler.sh -f put.html -e cpu,alloc,lock --chunktime 1s start Server`

## Тест 0

[wrk-put-report](wrk/wrk-put-report0), [wrk-get-report](wrk/wrk-get-report0)

Получилось, что сервис справляется со старыми цифрами (1 поток, 1 соединение, rate 30к/c для put, 10к/c для get)

Далее в тестах нагружаем следующим образом:

`wrk2 -c 64 -t 8 -d 2m -R 30000 -s put.lua --latency http://localhost:19234 > wrk-put-report0`

`wrk2 -c 64 -t 8 -d 2m -R 10000 -s get.lua --latency http://localhost:19234 > wrk-get-report0`

* 64 соединений
* 8 потоков
* 2 минуты
* 10000 запросов/сек для get
* 30000 запросов/сек для put

## Тест 1

`fifoRareness = 3` - проверим нашу очередь в режиме, когда каждый третий запрос обрабатывается в режиме FIFO

[wrk-put-report](wrk/wrk-put-report1_v2), [wrk-get-report](wrk/wrk-get-report1)

[сpu](html/cpu1_v2.html), [alloc](html/alloc1_v2.html), [lock](html/lock1_v2.html)

Получили для put:

* Средняя задержка в `1.32ms`
* `90.0%` перцентиль в `4.48ms`
* `99.0%` - `6.88ms`
* `99.9%` - `16.61ms`

Для get:

* Средняя задержка `1.03ms`
* `90.0%` перцентиль `2.05ms`
* `99.0%` - `2.52ms`
* `99.9%` - `7.60ms`

По сравнению со stage1 при профилировании наблюдаем:

* Отсутствие работы с dao в selector threads
* На `Lock` видим `69%` сэмплов queue.take() и `26.81%` queue.offer()
* На `CPU` видим `17.75%` сэмплов queue.take(), `1.76%` queue.offer()

## Тест 2

`fifoRareness = 1` - проверим нашу очередь в режиме, когда каждый запрос обрабатывается в режиме FIFO

[wrk-put-report](wrk/wrk-put-report2), [wrk-get-report](wrk/wrk-get-report2)

[сpu](html/cpu2.html), [alloc](html/alloc2.html), [lock](html/lock2.html)

Получили:

* Среднюю задержку в `1.27ms` для put запросов.
* Среднюю задержку в `0.96ms` для get запросов.
* Примерно такие же значения перцентилей
* При обычной нагрузке не видим снижения производительности ввиду эффекта, обсуждаемого на занятии, когда пока мы
  обрабатывали запросы, следующие затаймаутились, обработав их, затаймаутились следующие.

## Тест 3

Уменьшим размер очереди до `64`

[wrk-put-report](wrk/wrk-put-report3), [wrk-get-report](wrk/wrk-get-report3),

Видим:

* Средние задержки примерно те же
* Несколько бОльшие значения перцентилей для `get`
* Для put начиная с перцентиля `99.6%` видим задержку в `9ms` (что почти максимум для предыдущего теста)
* Для put после `99.6%` речь идёт уже о `десятках ms` задержки

Cкорее всего причина в том, что появляются потоки, которые блокируются на очереди. Проверим это, уменьшив очередь ещё
сильнее.

## Тест 4

Уменьшим размер очереди до `32`

[wrk-put-report](wrk/wrk-put-report4), [wrk-get-report](wrk/wrk-get-report4)

[сpu](html/cpu4.html), [alloc](html/alloc4.html), [lock](html/lock4.html)

На этой цифре мы видим, как производительность сервиса для перцентилей >70% cильно упала.
Получается эффект, когда потоки борются за очередь, в результате делают сильно меньше полезной работы, и heatMap по
большей части бледно-розовая.

Для get имеем:

* `75%` перцентиль - `2ms` задержки
* После перцентиля `77.5%` задержки пошли величиной в `секунду`
* После `80%` задержки уже `десятки секунд`
* На `90%` задержка уже `полминуты`

Для put имеем:

* `65%` перцентиль - `2ms` задержки
* Задержка в `1s` для перцентиля `70%`
* После `75%` задержки уже `десятки секунд`
* Для `90%` задержка `40s`

При профилировании видим:

* 23 - 24% всех сэмплов, воркеры тратят на queue.take, а селекторы на doSelect() (`17.75%` и `29.87%` для теста 1
  соответственно)
* На queue.offer() `1.71%` сэмплов (`2.41%` для теста 1)
* На `Lock` `70к` сэмплов для queue.take() (`140к` для теста 1), `54к` для queue.offer() (`57к` для теста 1)

### Сравнение с предыдущей реализаций

Проверим две реализации сервиса под нагрузкой:
`1 Мб` flushThreshold, put Rate = `60000`, get Rate = `45000`

[wrk-put-report-new](wrk/compare/wrk-put-report-new), [wrk-put-report-old](wrk/compare/wrk-put-report-old)

[wrk-get-report-new](wrk/compare/wrk-get-report-new_v2), [wrk-get-report-old](wrk/compare/wrk-get-report-old_v2)

Получили, что в сравнении со старой реализацией:

* Cредняя задержка для для put меньше в `2 раза`, а каждый из перцентилей меньше в `1.5-2 раза`
* Cредняя задержка для get меньше в `200 раз`, а для перцентилей задержки составляют вполне приемлемые
  значения (`2.97ms` для 99%, `12.67ms` для 99.9%)

Cтарая реализация для get не справилась с нагрузкой: значения перцентилей вплоть до примерно 90% те же, а после заметен
резкий скачок до неприемлемых задержек:

* Задержка в `18.43ms` уже наблюдается c перцентиля `88.85%` (подобная задержка для новой реализации видна только
  на `99.9%+`)
* `81.98ms` на `90%` (Для новой это `2.08ms` на том же перцентиле)
* Задержка в `1s` уже на `95%`
* `7s` для `99%`
* Начиная с `99.9%` речь уже пошла о `десятках секунд` задержки

### Итого

Освободив selector threads от работы с dao, мы получили лучшую работу сервиса под нагрузкой, что в особенности сказалось
на get запросах






