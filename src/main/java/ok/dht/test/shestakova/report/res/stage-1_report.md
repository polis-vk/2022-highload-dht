### **STAGE 1**

Для выполнения работы была использована своя реализацию DAO из весеннего курса 2022-nosql-lsm.

#### **Нагрузочное тестирование**
##### PUT-запросы
Для определения стабильной нагрузки параметр -R (rate) варьировался следующим образом: { 1000, 10000, 20000, 30000, 40000, 50000 }.
Среднее значение latency при искомом параметре, равном 50000, достигало 1.94 с, при 46000 - 2.89 мс, а при 40000 - менее одной миллисекунды. Таким образом было выбрано значение **rate = 40000, при котором интересующий нас параметр latency avg стал принимать значение 0.86 мс**.
На 90% запросов затрачивалось не более 1.01 мс при rate = 40000, а при увеличении rate = 41000 на 90% запросов затрачивалось около 1.0 мс, но сервис уже не справляется с этой нагрузкой (вылетает ошибка - таймаут), из чего можно сделать вывод о том, что первый вариант можно считать рабочей нагрузкой.

`wrk -t 1 -c 1 -d 60 -R 40000 -s put.lua -L http://localhost:19234`  
Running 1m test @ http://localhost:19234  
1 threads and 1 connections  
Thread calibration: mean lat.: 56.936ms, rate sampling interval: 280ms  
Thread Stats   Avg      Stdev     Max   +/- Stdev  
Latency     0.86ms    2.72ms  39.52ms   98.44%  
Req/Sec    40.07k   553.30    43.48k    97.19%  
Latency Distribution (HdrHistogram - Recorded Latency)  
50.000%  572.00us  
75.000%  844.00us  
90.000%    1.01ms  
99.000%   11.12ms  
99.900%   37.34ms  
99.990%   39.17ms  
99.999%   39.52ms  
100.000%   39.55ms  

Detailed Percentile spectrum:  
Value   Percentile   TotalCount 1/(1-Percentile)  

       0.018     0.000000           98         1.00
       0.132     0.100000       200344         1.11
       0.243     0.200000       400281         1.25
       0.354     0.300000       600983         1.43
       0.463     0.400000       800247         1.67
       0.572     0.500000      1000719         2.00
       0.627     0.550000      1101307         2.22
       0.681     0.600000      1200149         2.50
       0.736     0.650000      1300877         2.86
       0.790     0.700000      1400753         3.33
       0.844     0.750000      1500771         4.00
       0.871     0.775000      1550313         4.44
       0.898     0.800000      1599974         5.00
       0.926     0.825000      1651590         5.71
       0.953     0.850000      1701337         6.67
       0.980     0.875000      1751120         8.00
       0.993     0.887500      1775255         8.89
       1.007     0.900000      1801260        10.00
       1.020     0.912500      1825414        11.43
       1.034     0.925000      1851495        13.33
       1.047     0.937500      1875797        16.00
       1.054     0.943750      1888926        17.78
       1.060     0.950000      1900104        20.00
       1.067     0.956250      1913603        22.86
       1.073     0.962500      1926258        26.67
       1.125     0.968750      1937426        32.00
       1.220     0.971875      1943627        35.56
       1.418     0.975000      1949843        40.00
       1.822     0.978125      1956091        45.71
       2.517     0.981250      1962340        53.33
       3.581     0.984375      1968588        64.00
       4.535     0.985938      1971714        71.11
       5.367     0.987500      1974842        80.00
       7.791     0.989062      1977958        91.43
      12.719     0.990625      1981085       106.67
      17.199     0.992188      1984217       128.00
      19.167     0.992969      1985774       142.22
      21.711     0.993750      1987351       160.00
      22.991     0.994531      1988899       182.86
      25.567     0.995313      1990458       213.33
      29.855     0.996094      1992020       256.00
      32.095     0.996484      1992803       284.44
      33.663     0.996875      1993590       320.00
      34.591     0.997266      1994392       365.71
      35.647     0.997656      1995192       426.67
      35.903     0.998047      1995928       512.00
      36.191     0.998242      1996328       568.89
      36.511     0.998437      1996717       640.00
      36.831     0.998633      1997102       731.43
      37.119     0.998828      1997551       853.33
      37.407     0.999023      1997889      1024.00
      37.631     0.999121      1998082      1137.78
      37.823     0.999219      1998324      1280.00
      37.855     0.999316      1998470      1462.86
      37.919     0.999414      1998749      1706.67
      37.951     0.999512      1998856      2048.00
      38.015     0.999561      1998990      2275.56
      38.047     0.999609      1999053      2560.00
      38.143     0.999658      1999180      2925.71
      38.271     0.999707      1999261      3413.33
      38.463     0.999756      1999347      4096.00
      38.591     0.999780      1999395      4551.11
      38.719     0.999805      1999442      5120.00
      38.943     0.999829      1999498      5851.43
      39.071     0.999854      1999548      6826.67
      39.135     0.999878      1999613      8192.00
      39.135     0.999890      1999613      9102.22
      39.167     0.999902      1999637     10240.00
      39.199     0.999915      1999663     11702.86
      39.263     0.999927      1999692     13653.33
      39.327     0.999939      1999727     16384.00
      39.327     0.999945      1999727     18204.44
      39.359     0.999951      1999747     20480.00
      39.359     0.999957      1999747     23405.71
      39.391     0.999963      1999758     27306.67
      39.423     0.999969      1999770     32768.00
      39.455     0.999973      1999783     36408.89
      39.455     0.999976      1999783     40960.00
      39.487     0.999979      1999792     46811.43
      39.519     0.999982      1999815     54613.33
      39.519     0.999985      1999815     65536.00
      39.519     0.999986      1999815     72817.78
      39.519     0.999988      1999815     81920.00
      39.519     0.999989      1999815     93622.86
      39.519     0.999991      1999815    109226.67
      39.551     0.999992      1999831    131072.00
      39.551     1.000000      1999831          inf

[Mean    =        0.856, StdDeviation   =        2.719]

[Max     =       39.520, Total count    =      1999831]

[Buckets =           27, SubBuckets     =         2048]

----------------------------------------------------------
2399984 requests in 1.00m, 153.35MB read  
Requests/sec:  39999.84  
Transfer/sec:      2.56MB  

`wrk -t 1 -c 1 -d 60 -R 41000 -s put.lua -L http://localhost:19234`  
Running 1m test @ http://localhost:19234  
1 threads and 1 connections  
Thread calibration: mean lat.: 7.729ms, rate sampling interval: 74ms  
Thread Stats   Avg      Stdev     Max   +/- Stdev  
Latency   632.98us  746.11us  15.82ms   97.96%  
Req/Sec    38.95k     9.51k   45.97k    94.37%  
Latency Distribution (HdrHistogram - Recorded Latency)  
50.000%  569.00us  
75.000%  840.00us  
90.000%    1.00ms  
99.000%    3.22ms  
99.900%   11.64ms  
99.990%   15.44ms  
99.999%   15.78ms  
100.000%   15.82ms  

Detailed Percentile spectrum:  
Value   Percentile   TotalCount 1/(1-Percentile)  

       0.018     0.000000           53         1.00
       0.131     0.100000       193477         1.11
       0.242     0.200000       388260         1.25
       0.351     0.300000       580720         1.43
       0.460     0.400000       773404         1.67
       0.569     0.500000       966625         2.00
       0.624     0.550000      1064508         2.22
       0.678     0.600000      1160745         2.50
       0.732     0.650000      1256937         2.86
       0.786     0.700000      1353610         3.33
       0.840     0.750000      1450782         4.00
       0.867     0.775000      1498938         4.44
       0.894     0.800000      1547368         5.00
       0.921     0.825000      1595325         5.71
       0.948     0.850000      1643570         6.67
       0.975     0.875000      1691458         8.00
       0.989     0.887500      1716570         8.89
       1.002     0.900000      1739907        10.00
       1.016     0.912500      1765015        11.43
       1.029     0.925000      1788442        13.33
       1.043     0.937500      1813731        16.00
       1.049     0.943750      1824535        17.78
       1.056     0.950000      1837206        20.00
       1.063     0.956250      1849857        22.86
       1.069     0.962500      1861011        26.67
       1.076     0.968750      1872995        32.00
       1.113     0.971875      1878689        35.56
       1.191     0.975000      1884762        40.00
       1.306     0.978125      1890789        45.71
       1.527     0.981250      1896814        53.33
       2.004     0.984375      1902852        64.00
       2.313     0.985938      1905868        71.11
       2.619     0.987500      1908891        80.00
       2.971     0.989062      1911918        91.43
       3.389     0.990625      1914929       106.67
       3.969     0.992188      1917951       128.00
       4.287     0.992969      1919471       142.22
       4.587     0.993750      1920977       160.00
       5.023     0.994531      1922481       182.86
       5.535     0.995313      1923992       213.33
       6.111     0.996094      1925505       256.00
       6.415     0.996484      1926257       284.44
       6.759     0.996875      1927014       320.00
       7.147     0.997266      1927767       365.71
       7.727     0.997656      1928520       426.67
       8.831     0.998047      1929274       512.00
       9.303     0.998242      1929660       568.89
       9.687     0.998437      1930032       640.00
      10.279     0.998633      1930410       731.43
      11.143     0.998828      1930786       853.33
      11.711     0.999023      1931164      1024.00
      11.959     0.999121      1931352      1137.78
      12.271     0.999219      1931540      1280.00
      12.735     0.999316      1931736      1462.86
      13.199     0.999414      1931918      1706.67
      13.647     0.999512      1932108      2048.00
      13.919     0.999561      1932202      2275.56
      14.127     0.999609      1932296      2560.00
      14.383     0.999658      1932389      2925.71
      14.583     0.999707      1932485      3413.33
      14.831     0.999756      1932578      4096.00
      14.935     0.999780      1932627      4551.11
      15.039     0.999805      1932673      5120.00
      15.143     0.999829      1932719      5851.43
      15.223     0.999854      1932770      6826.67
      15.343     0.999878      1932815      8192.00
      15.407     0.999890      1932838      9102.22
      15.447     0.999902      1932862     10240.00
      15.511     0.999915      1932885     11702.86
      15.543     0.999927      1932910     13653.33
      15.559     0.999939      1932933     16384.00
      15.575     0.999945      1932946     18204.44
      15.599     0.999951      1932956     20480.00
      15.631     0.999957      1932968     23405.71
      15.663     0.999963      1932981     27306.67
      15.695     0.999969      1932991     32768.00
      15.711     0.999973      1932998     36408.89
      15.727     0.999976      1933004     40960.00
      15.743     0.999979      1933010     46811.43
      15.759     0.999982      1933017     54613.33
      15.767     0.999985      1933022     65536.00
      15.775     0.999986      1933028     72817.78
      15.775     0.999988      1933028     81920.00
      15.783     0.999989      1933036     93622.86
      15.783     0.999991      1933036    109226.67
      15.783     0.999992      1933036    131072.00
      15.783     0.999993      1933036    145635.56
      15.791     0.999994      1933038    163840.00
      15.799     0.999995      1933040    187245.71
      15.807     0.999995      1933045    218453.33
      15.807     0.999996      1933045    262144.00
      15.807     0.999997      1933045    291271.11
      15.807     0.999997      1933045    327680.00
      15.807     0.999997      1933045    374491.43
      15.807     0.999998      1933045    436906.67
      15.815     0.999998      1933048    524288.00
      15.815     0.999998      1933048    582542.22
      15.815     0.999998      1933048    655360.00
      15.815     0.999999      1933048    748982.86
      15.815     0.999999      1933048    873813.33
      15.815     0.999999      1933048   1048576.00
      15.815     0.999999      1933048   1165084.44
      15.815     0.999999      1933048   1310720.00
      15.815     0.999999      1933048   1497965.71
      15.815     0.999999      1933048   1747626.67
      15.823     1.000000      1933049   2097152.00
      15.823     1.000000      1933049          inf

[Mean    =        0.633, StdDeviation   =        0.746]

[Max     =       15.816, Total count    =      1933049]

[Buckets =           27, SubBuckets     =         2048]

----------------------------------------------------------  
2343212 requests in 1.00m, 149.72MB read  
Socket errors: connect 0, read 0, write 0, timeout 1  
Requests/sec:  39050.34  
Transfer/sec:      2.50MB  


##### GET-запросы на заполненной (1.4 Гб) БД
Для определения стабильной нагрузки параметр -R (rate) варьировался следующим образом: { 1000, 2000, 3000, 4000, 5000, 6000, 10000 }.
Среднее значение latency при искомом параметре, равном 10000, достигало 2.76 с, при 6000 - 1.10 мс, а **при 5000 - 0.86 мс**.
На 90% запросов затрачивалось не более 1.29 мс при rate = 5000, 2.0 мс при rate = 6000, из чего можно сделать вывод о том, что первый вариант можно считать рабочей нагрузкой.

UPD:
При вышеупомянутых рассчетах БД содержала недостаточно большое количество данных, далее БД была заполнена на 1.4 Гб, в связи с чем серверу пришлось искать искомые ключи в большем количестве файлов на диске, в каждом просматриваемом файле искать бинарным поискам нужный ключ и тд, в связи с чем показатели значительно ухудшились: при rate = 100 среднее значение latency было 5.93 мс, а при rate = 1000 - уже 18.88 мс. Причем в первом варианте 90% запросов обработывалось с latency <= 7.07 мс, максимально - 9.05 мс, а при rate = 1000: 90% - 96.64 мс, а 100% - уже 137.09 мс. 

`wrk -t 1 -c 1 -d 30 -R 100 -s get.lua -L http://localhost:19234`  
Running 30s test @ http://localhost:19234  
1 threads and 1 connections  
Thread calibration: mean lat.: 5.830ms, rate sampling interval: 13ms  
Thread Stats   Avg      Stdev     Max   +/- Stdev  
Latency     5.93ms    0.96ms   9.04ms   74.35%  
Req/Sec   103.41     36.51   166.00     67.99%  
Latency Distribution (HdrHistogram - Recorded Latency)  
50.000%    6.00ms  
75.000%    6.47ms  
90.000%    7.07ms  
99.000%    7.85ms  
99.900%    8.23ms  
99.990%    9.05ms  
99.999%    9.05ms  
100.000%    9.05ms  

Detailed Percentile spectrum:  
Value   Percentile   TotalCount 1/(1-Percentile)  

       1.740     0.000000            1         1.00
       4.767     0.100000          201         1.11
       5.335     0.200000          402         1.25
       5.623     0.300000          600         1.43
       5.811     0.400000          801         1.67
       5.999     0.500000         1002         2.00
       6.071     0.550000         1101         2.22
       6.159     0.600000         1201         2.50
       6.267     0.650000         1303         2.86
       6.359     0.700000         1400         3.33
       6.471     0.750000         1501         4.00
       6.543     0.775000         1553         4.44
       6.619     0.800000         1601         5.00
       6.699     0.825000         1651         5.71
       6.791     0.850000         1701         6.67
       6.907     0.875000         1751         8.00
       6.987     0.887500         1775         8.89
       7.071     0.900000         1801        10.00
       7.151     0.912500         1826        11.43
       7.255     0.925000         1850        13.33
       7.335     0.937500         1876        16.00
       7.383     0.943750         1888        17.78
       7.435     0.950000         1900        20.00
       7.483     0.956250         1913        22.86
       7.563     0.962500         1926        26.67
       7.627     0.968750         1939        32.00
       7.643     0.971875         1945        35.56
       7.659     0.975000         1951        40.00
       7.683     0.978125         1957        45.71
       7.731     0.981250         1963        53.33
       7.763     0.984375         1970        64.00
       7.783     0.985938         1973        71.11
       7.795     0.987500         1975        80.00
       7.835     0.989062         1979        91.43
       7.863     0.990625         1983       106.67
       7.895     0.992188         1985       128.00
       7.899     0.992969         1987       142.22
       7.903     0.993750         1988       160.00
       7.911     0.994531         1990       182.86
       7.923     0.995313         1991       213.33
       8.007     0.996094         1993       256.00
       8.007     0.996484         1993       284.44
       8.031     0.996875         1994       320.00
       8.055     0.997266         1996       365.71
       8.055     0.997656         1996       426.67
       8.079     0.998047         1997       512.00
       8.079     0.998242         1997       568.89
       8.079     0.998437         1997       640.00
       8.231     0.998633         1998       731.43
       8.231     0.998828         1998       853.33
       8.519     0.999023         1999      1024.00
       8.519     0.999121         1999      1137.78
       8.519     0.999219         1999      1280.00
       8.519     0.999316         1999      1462.86
       8.519     0.999414         1999      1706.67
       9.047     0.999512         2000      2048.00
       9.047     1.000000         2000          inf

[Mean    =        5.926, StdDeviation   =        0.957]

[Max     =        9.040, Total count    =         2000]

[Buckets =           27, SubBuckets     =         2048]

----------------------------------------------------------  
3000 requests in 30.01s, 195.21KB read  
Requests/sec:     99.98  
Transfer/sec:      6.51KB  

С увеличением rate с 100 до 1000 среднее значение latency возрастает более, чем в 3 раза.

`wrk -t 1 -c 1 -d 30 -R 1000 -s get.lua -L http://localhost:19234`  
Running 30s test @ http://localhost:19234  
1 threads and 1 connections  
Thread calibration: mean lat.: 4.427ms, rate sampling interval: 21ms  
Thread Stats   Avg      Stdev     Max   +/- Stdev  
Latency    18.88ms   37.35ms 136.96ms   87.49%  
Req/Sec     1.03k   108.08     1.25k    82.00%  
Latency Distribution (HdrHistogram - Recorded Latency)  
50.000%    2.09ms  
75.000%   10.88ms  
90.000%   96.64ms  
99.000%  134.78ms  
99.900%  136.83ms  
99.990%  137.09ms  
99.999%  137.09ms  
100.000%  137.09ms  

Detailed Percentile spectrum:  
Value   Percentile   TotalCount 1/(1-Percentile)  

       0.801     0.000000            1         1.00
       1.139     0.100000         2004         1.11
       1.376     0.200000         4001         1.25
       1.600     0.300000         6002         1.43
       1.811     0.400000         8000         1.67
       2.091     0.500000         9998         2.00
       2.629     0.550000        10999         2.22
       4.407     0.600000        11998         2.50
       6.347     0.650000        12998         2.86
       8.735     0.700000        13999         3.33
      10.879     0.750000        15000         4.00
      11.895     0.775000        15499         4.44
      12.807     0.800000        15998         5.00
      14.279     0.825000        16498         5.71
      28.607     0.850000        16998         6.67
      56.415     0.875000        17497         8.00
      76.351     0.887500        17747         8.89
      96.767     0.900000        17997        10.00
     110.911     0.912500        18247        11.43
     118.399     0.925000        18497        13.33
     123.583     0.937500        18752        16.00
     124.095     0.943750        18891        17.78
     124.415     0.950000        19004        20.00
     124.991     0.956250        19136        22.86
     125.887     0.962500        19248        26.67
     128.447     0.968750        19373        32.00
     128.959     0.971875        19444        35.56
     129.151     0.975000        19500        40.00
     129.663     0.978125        19561        45.71
     130.303     0.981250        19630        53.33
     131.327     0.984375        19685        64.00
     132.991     0.985938        19715        71.11
     133.887     0.987500        19747        80.00
     134.527     0.989062        19779        91.43
     134.911     0.990625        19814       106.67
     135.295     0.992188        19842       128.00
     135.423     0.992969        19856       142.22
     135.679     0.993750        19876       160.00
     135.935     0.994531        19905       182.86
     135.935     0.995313        19905       213.33
     136.191     0.996094        19926       256.00
     136.191     0.996484        19926       284.44
     136.319     0.996875        19937       320.00
     136.447     0.997266        19945       365.71
     136.575     0.997656        19962       426.67
     136.575     0.998047        19962       512.00
     136.575     0.998242        19962       568.89
     136.703     0.998437        19970       640.00
     136.703     0.998633        19970       731.43
     136.831     0.998828        19978       853.33
     136.831     0.999023        19978      1024.00
     136.959     0.999121        19989      1137.78
     136.959     0.999219        19989      1280.00
     136.959     0.999316        19989      1462.86
     136.959     0.999414        19989      1706.67
     136.959     0.999512        19989      2048.00
     136.959     0.999561        19989      2275.56
     136.959     0.999609        19989      2560.00
     137.087     0.999658        19996      2925.71
     137.087     1.000000        19996          inf

[Mean    =       18.882, StdDeviation   =       37.353]

[Max     =      136.960, Total count    =        19996]

[Buckets =           27, SubBuckets     =         2048]

----------------------------------------------------------  
29999 requests in 30.00s, 1.93MB read  
Requests/sec:    999.97  
Transfer/sec:     65.72KB

#### **Профилирование (CPU и alloc) с помощью async-profiler**
##### PUT-запросы
**CPU**
![Иллюстрация к проекту](https://github.com/Anilochka/2022-highload-dht/blob/main/src/main/java/ok/dht/test/shestakova/report/jpg/put_cpu.jpg)

- большая часть процессорного времени идёт на работу ядра (работа с сетевым протоколом, syscalls), запросами (one.nio);
- ~3% затрачивается на работу с MemorySegment;
- ~1% времени идёт на работу БД.

**alloc**
![Иллюстрация к проекту](https://github.com/Anilochka/2022-highload-dht/blob/main/src/main/java/ok/dht/test/shestakova/report/jpg/put_alloc.jpg)

Исходя из полученных результатов, можно подытожить, что 
- ~9% памяти затрачивается на работу с MemorySegment;
- ~7% памяти затрачивается на обработку запроса put реализованным DemoService;
- ~27% памяти затрачивается на работу java.lang, java.util (массивы, concurrency - put в SkipListMap);
- остальная память идёт на работу one.nio (работу с массивом байт, сессиями, а также создание и обработку запросов). 

Сервер читает данные из сокета в массив байт, вырезает подмассив (9% - ByteArrayBuilder, 8.5% - Arrays.copyOfRange : использующиеся в one.nio в методе processHttpBuffer, чтобы записать в массив-буффер часть тела запроса), а затем в нашем сервере мы хотим получить из массива байт строку, которую далее обратно превращаем в массив байт для дальнейшего использования в качестве ключа в нашей БД. Кроме того далее мы создаем новый Response, который далее будет конвертироваться в массив байт для дальнейшей передачи в сокет. Т.е. стоит оптимизировать работу с получаемыми и передаваемыми сокету данными, чтобы избежать лишних промежуточных аллокаций для создания строк и Response.
~4% уходит на фоновый flush, среди которых 2,6% уходят на метод grow ArrayList-a, используемого для хранения записей из итератора.
Возможно, стоит увеличить размер flushThreshold, чтобы реже писать данные на диск.
Ещё (как уже сказали на лекции) стоит распределить нагрузку, связанную с фоновым flush более равномерно, разгрузив SelectorThread-ы. 

##### GET-запросы
**CPU**
![Иллюстрация к проекту](https://github.com/Anilochka/2022-highload-dht/blob/main/src/main/java/ok/dht/test/shestakova/report/jpg/get_cpu.jpg)

- 74% уходит на метод iterate для поиска ключа по файлам, в каждом из которых 55% из этого времени уходит только на метод greaterOrEqualEntryIndex - бинарный поиск по файлу.
- остальная часть процессорного времени идёт на работу ядра (работа с сетевым протоколом, syscalls), а также работу с зпросами.

**alloc**
![Иллюстрация к проекту](https://github.com/Anilochka/2022-highload-dht/blob/main/src/main/java/ok/dht/test/shestakova/report/jpg/get_alloc.jpg)

Исходя из полученных результатов, можно подытожить, что 
- ~93% памяти затрачивается на поиск ключа на диске, около 5% на поиск ключа в оперативной памяти;
- остальная память идёт на работу one.nio (работу с массивом байт, сессиями, а также создание и обработку запросов). 

Надо оптимизировать поиск ключа на диске: 
Первой мыслью было добавить фильтр Блума, но из-за того, что в вышеприведённом эксперименте мы запрашиваем id, которые точно лежат в БД, эта идея не подойдёт. Хотя это можно добавить на случай, если бы мы передавали в get-запрос рандомный id для поиска.  
Наверное стоит увеличить размер flushThreshold для более оптимального бинарного поиска.
Может, стоит делать фоновый compaction для уменьшения количества записей (повторных), удаления "могилок", да и в целом для того, чтобы бинарный поиск не ходил во много маленьких файлов.
Также можно поискать более производительные коллекции для хранения ключей в оперативной памяти вместо ConcurrentSkipListMap. (?)

